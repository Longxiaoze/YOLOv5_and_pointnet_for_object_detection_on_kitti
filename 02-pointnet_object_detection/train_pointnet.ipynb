{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 10 18:12:22 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.119.03   Driver Version: 450.119.03   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 206...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   56C    P5     9W /  N/A |    885MiB /  5934MiB |     28%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1091      G   /usr/lib/xorg/Xorg                322MiB |\n",
      "|    0   N/A  N/A      1510      G   /usr/bin/gnome-shell              195MiB |\n",
      "|    0   N/A  N/A      2276      G   ...AAAAAAAAA= --shared-files      135MiB |\n",
      "|    0   N/A  N/A      7070      G   ...AAAAAAAAA= --shared-files       58MiB |\n",
      "|    0   N/A  N/A      7193      G   ...AAAAAAAA== --shared-files      167MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./KITTI/object/object_training_datasets\"\n",
    "file_list = glob.glob(os.path.join(root,\"*/*.txt\"))\n",
    "cls_names = [p.split(\"/\")[7] for p in file_list]\n",
    "cls_names = np.unique(cls_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsname_to_index = dict((name,index) for index,name in enumerate(cls_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_clsname = dict((index,name) for name,index in clsname_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_shuffle = np.random.permutation(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = np.asarray(file_list)[index_shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = file_list[:int(0.8*N)]\n",
    "test_list = file_list[int(0.8*N):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = list(train_list)\n",
    "test_paths = list(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [clsname_to_index.get(p.split(\"/\")[7]) for p in train_paths]\n",
    "test_labels = [clsname_to_index.get(p.split(\"/\")[7]) for p in test_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = tf.data.Dataset.from_tensor_slices((train_paths,train_labels))\n",
    "test_datasets = tf.data.Dataset.from_tensor_slices((test_paths,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_fun(path,label):\n",
    "  # 读取点云文件\n",
    "  point_cloud_path = path.numpy()\n",
    "  point_cloud_path = point_cloud_path.decode()\n",
    "  point_cloud = pd.read_csv(point_cloud_path)\n",
    "  point_cloud = np.array(point_cloud.iloc[:,0:6])\n",
    "\n",
    "  # 归一化\n",
    "  point_cloud[:,0:3] = point_cloud[:,0:3] - np.expand_dims(np.mean(point_cloud[:,0:3],0),0)\n",
    "  dist = np.max(np.sqrt(np.sum(point_cloud[:,0:3]**2,axis=1)),0)\n",
    "  if(dist > 0.0001):\n",
    "    point_cloud[:,0:3] = point_cloud[:,0:3]/dist\n",
    "\n",
    "  # 绕Z轴旋转随机角度\n",
    "  theta = np.random.uniform(0,np.pi*2)\n",
    "  rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\n",
    "  point_cloud[:,[0,1]] = point_cloud[:,[0,1]].dot(rotation_matrix)\n",
    "  point_cloud[:,[3,4]] = point_cloud[:,[3,4]].dot(rotation_matrix)\n",
    "\n",
    "  point_cloud = tf.convert_to_tensor(point_cloud,dtype=tf.float64)\n",
    "\n",
    "  # 添加噪声\n",
    "  point_cloud += tf.random.uniform(point_cloud.shape, -0.005, 0.005, dtype=tf.float64)\n",
    "  # 随机乱序\n",
    "  point_cloud = tf.random.shuffle(point_cloud)\n",
    "\n",
    "  label = tf.cast(label,dtype=tf.int64)\n",
    "  return point_cloud,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(x,y):\n",
    "    x, y = tf.py_function(load_train_fun, inp=[x, y], Tout=[tf.float64, tf.int64])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_fun(path,label):\n",
    "  # 读取点云文件\n",
    "  point_cloud_path = path.numpy()\n",
    "  point_cloud_path = point_cloud_path.decode()\n",
    "  point_cloud = pd.read_csv(point_cloud_path)\n",
    "  point_cloud = np.array(point_cloud.iloc[:,0:6])\n",
    "\n",
    "  # 归一化\n",
    "  point_cloud[:,0:3] = point_cloud[:,0:3] - np.expand_dims(np.mean(point_cloud[:,0:3],0),0)\n",
    "  dist = np.max(np.sqrt(np.sum(point_cloud[:,0:3]**2,axis=1)),0)\n",
    "  if(dist > 0.0001):\n",
    "    point_cloud[:,0:3] = point_cloud[:,0:3]/dist\n",
    "\n",
    "  point_cloud = tf.convert_to_tensor(point_cloud,dtype=tf.float64)\n",
    "  label = tf.cast(label,dtype=tf.int64)\n",
    "  return point_cloud,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(x,y):\n",
    "    x, y = tf.py_function(load_test_fun, inp=[x, y], Tout=[tf.float64, tf.int64])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = train_datasets.shuffle(len(train_paths))\n",
    "test_datasets = test_datasets.shuffle(len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = train_datasets.map(load_train)\n",
    "test_datasets = test_datasets.map(load_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POINTS = 64\n",
    "NUM_CLASSES = 40\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = train_datasets.batch(BATCH_SIZE)\n",
    "test_datasets = test_datasets.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(x, filters):\n",
    "    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)\n",
    "\n",
    "\n",
    "def dense_bn(x, filters):\n",
    "    x = layers.Dense(filters)(x)\n",
    "    x = layers.BatchNormalization(momentum=0.0)(x)\n",
    "    return layers.Activation(\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来保证矩阵的秩为1\n",
    "class OrthogonalRegularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, num_features, l2reg=0.001):\n",
    "        self.num_features = num_features\n",
    "        self.l2reg = l2reg\n",
    "        self.eye = tf.eye(num_features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n",
    "        xxt = tf.tensordot(x, x, axes=(2, 2))\n",
    "        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n",
    "        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n",
    "\n",
    "    def get_config(self):\n",
    "      return {'num_features': int(self.num_features), 'l2reg': float(self.l2reg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tnet(inputs, num_features):\n",
    "\n",
    "    # Initalise bias as the indentity matrix\n",
    "    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n",
    "    reg = OrthogonalRegularizer(num_features)\n",
    "  \n",
    "    x = conv_bn(inputs, 64)\n",
    "    x = conv_bn(x, 128)\n",
    "    x = conv_bn(x, 1024)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = dense_bn(x, 512)\n",
    "    x = dense_bn(x, 256)\n",
    "    x = layers.Dense(\n",
    "        num_features * num_features,\n",
    "        kernel_initializer=\"zeros\",\n",
    "        bias_initializer=bias,\n",
    "        activity_regularizer=reg,\n",
    "    )(x)\n",
    "    feat_T = layers.Reshape((num_features, num_features))(x)\n",
    "    # Apply affine transformation to input features\n",
    "    return layers.Dot(axes=(2, 1))([inputs, feat_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pointnet\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64, 6)]           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 64, 64)            448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 64, 64)            4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 64, 64)            4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 64, 128)           8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 64, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 64, 1024)          132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 64, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 64, 1024)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 40)                10280     \n",
      "=================================================================\n",
      "Total params: 824,040\n",
      "Trainable params: 819,816\n",
      "Non-trainable params: 4,224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(NUM_POINTS, 6))\n",
    "\n",
    "#x = tnet(inputs, 3)\n",
    "x = conv_bn(inputs, 64)\n",
    "x = conv_bn(x, 64)\n",
    "#x = tnet(x, 64)\n",
    "x = conv_bn(x, 64)\n",
    "x = conv_bn(x, 128)\n",
    "x = conv_bn(x, 1024)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = dense_bn(x, 512)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = dense_bn(x, 256)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(\"logs\",datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenserboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "2671/2671 [==============================] - 235s 88ms/step - loss: 0.7627 - sparse_categorical_accuracy: 0.7166 - val_loss: 1.9081 - val_sparse_categorical_accuracy: 0.6965\n",
      "Epoch 2/14\n",
      "2671/2671 [==============================] - 232s 87ms/step - loss: 0.4487 - sparse_categorical_accuracy: 0.8316 - val_loss: 8.4835 - val_sparse_categorical_accuracy: 0.4126\n",
      "Epoch 3/14\n",
      "2671/2671 [==============================] - 231s 86ms/step - loss: 0.3728 - sparse_categorical_accuracy: 0.8621 - val_loss: 0.9890 - val_sparse_categorical_accuracy: 0.8266\n",
      "Epoch 4/14\n",
      "2671/2671 [==============================] - 231s 86ms/step - loss: 0.3318 - sparse_categorical_accuracy: 0.8780 - val_loss: 1.5151 - val_sparse_categorical_accuracy: 0.7907\n",
      "Epoch 5/14\n",
      "2671/2671 [==============================] - 232s 87ms/step - loss: 0.2934 - sparse_categorical_accuracy: 0.8927 - val_loss: 6.2497 - val_sparse_categorical_accuracy: 0.5429\n",
      "Epoch 6/14\n",
      "2671/2671 [==============================] - 231s 86ms/step - loss: 0.2713 - sparse_categorical_accuracy: 0.9030 - val_loss: 1.6877 - val_sparse_categorical_accuracy: 0.7787\n",
      "Epoch 7/14\n",
      "2671/2671 [==============================] - 232s 87ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9149 - val_loss: 7.8005 - val_sparse_categorical_accuracy: 0.4878\n",
      "Epoch 8/14\n",
      "2671/2671 [==============================] - 234s 87ms/step - loss: 0.2241 - sparse_categorical_accuracy: 0.9197 - val_loss: 0.8816 - val_sparse_categorical_accuracy: 0.8468\n",
      "Epoch 9/14\n",
      "2671/2671 [==============================] - 232s 87ms/step - loss: 0.2050 - sparse_categorical_accuracy: 0.9253 - val_loss: 1.6327 - val_sparse_categorical_accuracy: 0.7887\n",
      "Epoch 10/14\n",
      "2671/2671 [==============================] - 230s 86ms/step - loss: 0.1850 - sparse_categorical_accuracy: 0.9311 - val_loss: 2.4778 - val_sparse_categorical_accuracy: 0.7289\n",
      "Epoch 11/14\n",
      "2671/2671 [==============================] - 230s 86ms/step - loss: 0.1874 - sparse_categorical_accuracy: 0.9306 - val_loss: 2.6665 - val_sparse_categorical_accuracy: 0.7247\n",
      "Epoch 12/14\n",
      "2671/2671 [==============================] - 230s 86ms/step - loss: 0.1680 - sparse_categorical_accuracy: 0.9395 - val_loss: 2.1122 - val_sparse_categorical_accuracy: 0.7802\n",
      "Epoch 13/14\n",
      "2671/2671 [==============================] - 229s 86ms/step - loss: 0.1597 - sparse_categorical_accuracy: 0.9421 - val_loss: 4.2442 - val_sparse_categorical_accuracy: 0.6858\n",
      "Epoch 14/14\n",
      "2671/2671 [==============================] - 230s 86ms/step - loss: 0.1563 - sparse_categorical_accuracy: 0.9445 - val_loss: 0.6090 - val_sparse_categorical_accuracy: 0.8739\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(train_datasets, epochs=14, validation_data=test_datasets,callbacks=[tenserboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"pointnet_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 133072), started 1:49:13 ago. (Use '!kill 133072' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-54395e8ebbfcbe7d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-54395e8ebbfcbe7d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'cyclist', 1: 'misc', 2: 'pedestrian', 3: 'vehicle'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_clsname"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "962b00edaefde94591e29a293936598abdbbd5e5bc848f9b359e207bb81234a1"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
